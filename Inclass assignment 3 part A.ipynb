{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80516c42",
   "metadata": {},
   "source": [
    "C861333 - Thulana Abeywardane\n",
    "\n",
    "C860583 - Praveen Mahaulpatha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bbcb6e",
   "metadata": {},
   "source": [
    "Implement following pre-processing on the text given below:  (Submit a zipped file of original Ipynb file + html)\n",
    "\n",
    "1- Remove stopword and punctuation\n",
    "\n",
    "2- Apply stemming & lemmetization\n",
    "\n",
    "3- POS\n",
    "\n",
    "4- Bag of words\n",
    "\n",
    "5- Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76ae2b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Feature Extraction aims to reduce the number of features in a dataset by creating new features from the existing ones (and then discarding the original features). These new reduced set of features should then be able to summarize most of the information contained in the original set of features!!!. In this way, a summarised version of the original features can be created from a combination of the original set!!!\n",
    "\n",
    "Another commonly used technique to reduce the number of feature in a dataset is Feature Selection! The difference between Feature Selection and/or Feature Extraction is that feature selection aims instead to $ rank the importance of the existing features in the dataset and discard less important ones (no new features are created)?!. If you are interested in finding out more about Feature Selection, you can find more information about it in my previous article.\n",
    "\n",
    "In this article, I will walk you through how to apply Feature Extraction techniques using the Kaggle Mushroom Classification Dataset as an example??? Our objective will be to try to predict if a Mushroom is poisonous or not by looking at the given features. All the code used in this post (and more!) is available on Kaggle and on my GitHub Account.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f568369",
   "metadata": {},
   "source": [
    "### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9fe648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3932938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceList = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4edc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentenceList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eccd17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adec7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = []\n",
    "\n",
    "for sent in sentenceList:\n",
    "    word_tokens.append(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c650ccf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Feature',\n",
       "  'Extraction',\n",
       "  'aims',\n",
       "  'to',\n",
       "  'reduce',\n",
       "  'the',\n",
       "  'number',\n",
       "  'of',\n",
       "  'features',\n",
       "  'in',\n",
       "  'a',\n",
       "  'dataset',\n",
       "  'by',\n",
       "  'creating',\n",
       "  'new',\n",
       "  'features',\n",
       "  'from',\n",
       "  'the',\n",
       "  'existing',\n",
       "  'ones',\n",
       "  '(',\n",
       "  'and',\n",
       "  'then',\n",
       "  'discarding',\n",
       "  'the',\n",
       "  'original',\n",
       "  'features',\n",
       "  ')',\n",
       "  '.'],\n",
       " ['These',\n",
       "  'new',\n",
       "  'reduced',\n",
       "  'set',\n",
       "  'of',\n",
       "  'features',\n",
       "  'should',\n",
       "  'then',\n",
       "  'be',\n",
       "  'able',\n",
       "  'to',\n",
       "  'summarize',\n",
       "  'most',\n",
       "  'of',\n",
       "  'the',\n",
       "  'information',\n",
       "  'contained',\n",
       "  'in',\n",
       "  'the',\n",
       "  'original',\n",
       "  'set',\n",
       "  'of',\n",
       "  'features',\n",
       "  '!',\n",
       "  '!',\n",
       "  '!',\n",
       "  '.'],\n",
       " ['In',\n",
       "  'this',\n",
       "  'way',\n",
       "  ',',\n",
       "  'a',\n",
       "  'summarised',\n",
       "  'version',\n",
       "  'of',\n",
       "  'the',\n",
       "  'original',\n",
       "  'features',\n",
       "  'can',\n",
       "  'be',\n",
       "  'created',\n",
       "  'from',\n",
       "  'a',\n",
       "  'combination',\n",
       "  'of',\n",
       "  'the',\n",
       "  'original',\n",
       "  'set',\n",
       "  '!',\n",
       "  '!',\n",
       "  '!'],\n",
       " ['Another',\n",
       "  'commonly',\n",
       "  'used',\n",
       "  'technique',\n",
       "  'to',\n",
       "  'reduce',\n",
       "  'the',\n",
       "  'number',\n",
       "  'of',\n",
       "  'feature',\n",
       "  'in',\n",
       "  'a',\n",
       "  'dataset',\n",
       "  'is',\n",
       "  'Feature',\n",
       "  'Selection',\n",
       "  '!'],\n",
       " ['The',\n",
       "  'difference',\n",
       "  'between',\n",
       "  'Feature',\n",
       "  'Selection',\n",
       "  'and/or',\n",
       "  'Feature',\n",
       "  'Extraction',\n",
       "  'is',\n",
       "  'that',\n",
       "  'feature',\n",
       "  'selection',\n",
       "  'aims',\n",
       "  'instead',\n",
       "  'to',\n",
       "  '$',\n",
       "  'rank',\n",
       "  'the',\n",
       "  'importance',\n",
       "  'of',\n",
       "  'the',\n",
       "  'existing',\n",
       "  'features',\n",
       "  'in',\n",
       "  'the',\n",
       "  'dataset',\n",
       "  'and',\n",
       "  'discard',\n",
       "  'less',\n",
       "  'important',\n",
       "  'ones',\n",
       "  '(',\n",
       "  'no',\n",
       "  'new',\n",
       "  'features',\n",
       "  'are',\n",
       "  'created',\n",
       "  ')',\n",
       "  '?',\n",
       "  '!',\n",
       "  '.'],\n",
       " ['If',\n",
       "  'you',\n",
       "  'are',\n",
       "  'interested',\n",
       "  'in',\n",
       "  'finding',\n",
       "  'out',\n",
       "  'more',\n",
       "  'about',\n",
       "  'Feature',\n",
       "  'Selection',\n",
       "  ',',\n",
       "  'you',\n",
       "  'can',\n",
       "  'find',\n",
       "  'more',\n",
       "  'information',\n",
       "  'about',\n",
       "  'it',\n",
       "  'in',\n",
       "  'my',\n",
       "  'previous',\n",
       "  'article',\n",
       "  '.'],\n",
       " ['In',\n",
       "  'this',\n",
       "  'article',\n",
       "  ',',\n",
       "  'I',\n",
       "  'will',\n",
       "  'walk',\n",
       "  'you',\n",
       "  'through',\n",
       "  'how',\n",
       "  'to',\n",
       "  'apply',\n",
       "  'Feature',\n",
       "  'Extraction',\n",
       "  'techniques',\n",
       "  'using',\n",
       "  'the',\n",
       "  'Kaggle',\n",
       "  'Mushroom',\n",
       "  'Classification',\n",
       "  'Dataset',\n",
       "  'as',\n",
       "  'an',\n",
       "  'example',\n",
       "  '?',\n",
       "  '?',\n",
       "  '?'],\n",
       " ['Our',\n",
       "  'objective',\n",
       "  'will',\n",
       "  'be',\n",
       "  'to',\n",
       "  'try',\n",
       "  'to',\n",
       "  'predict',\n",
       "  'if',\n",
       "  'a',\n",
       "  'Mushroom',\n",
       "  'is',\n",
       "  'poisonous',\n",
       "  'or',\n",
       "  'not',\n",
       "  'by',\n",
       "  'looking',\n",
       "  'at',\n",
       "  'the',\n",
       "  'given',\n",
       "  'features',\n",
       "  '.'],\n",
       " ['All',\n",
       "  'the',\n",
       "  'code',\n",
       "  'used',\n",
       "  'in',\n",
       "  'this',\n",
       "  'post',\n",
       "  '(',\n",
       "  'and',\n",
       "  'more',\n",
       "  '!',\n",
       "  ')'],\n",
       " ['is',\n",
       "  'available',\n",
       "  'on',\n",
       "  'Kaggle',\n",
       "  'and',\n",
       "  'on',\n",
       "  'my',\n",
       "  'GitHub',\n",
       "  'Account',\n",
       "  '.']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d032f3cd",
   "metadata": {},
   "source": [
    "### 2. Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "387b1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as sw\n",
    "stopwordsEnglish = sw.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6efcb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_removed = []\n",
    "\n",
    "for s in word_tokens:\n",
    "    tempArr = []\n",
    "    for word in s:\n",
    "        if word not in stopwordsEnglish:\n",
    "            tempArr.append(word)\n",
    "    stop_word_removed.append(tempArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc9cf4a",
   "metadata": {},
   "source": [
    "### 3. Apply stemming & lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0976f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer\n",
    "pst = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75173a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = []\n",
    "for s in stop_word_removed:\n",
    "    tempArr = []\n",
    "    for word in s:\n",
    "        tempArr.append(pst.stem(word))\n",
    "    stemmed_words.append(tempArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61af45e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['featur',\n",
       "  'extract',\n",
       "  'aim',\n",
       "  'reduc',\n",
       "  'number',\n",
       "  'featur',\n",
       "  'dataset',\n",
       "  'creat',\n",
       "  'new',\n",
       "  'featur',\n",
       "  'exist',\n",
       "  'one',\n",
       "  '(',\n",
       "  'discard',\n",
       "  'origin',\n",
       "  'featur',\n",
       "  ')',\n",
       "  '.'],\n",
       " ['these',\n",
       "  'new',\n",
       "  'reduc',\n",
       "  'set',\n",
       "  'featur',\n",
       "  'abl',\n",
       "  'summar',\n",
       "  'inform',\n",
       "  'contain',\n",
       "  'origin',\n",
       "  'set',\n",
       "  'featur',\n",
       "  '!',\n",
       "  '!',\n",
       "  '!',\n",
       "  '.'],\n",
       " ['in',\n",
       "  'way',\n",
       "  ',',\n",
       "  'summaris',\n",
       "  'version',\n",
       "  'origin',\n",
       "  'featur',\n",
       "  'creat',\n",
       "  'combin',\n",
       "  'origin',\n",
       "  'set',\n",
       "  '!',\n",
       "  '!',\n",
       "  '!'],\n",
       " ['anoth',\n",
       "  'commonli',\n",
       "  'use',\n",
       "  'techniqu',\n",
       "  'reduc',\n",
       "  'number',\n",
       "  'featur',\n",
       "  'dataset',\n",
       "  'featur',\n",
       "  'select',\n",
       "  '!'],\n",
       " ['the',\n",
       "  'differ',\n",
       "  'featur',\n",
       "  'select',\n",
       "  'and/or',\n",
       "  'featur',\n",
       "  'extract',\n",
       "  'featur',\n",
       "  'select',\n",
       "  'aim',\n",
       "  'instead',\n",
       "  '$',\n",
       "  'rank',\n",
       "  'import',\n",
       "  'exist',\n",
       "  'featur',\n",
       "  'dataset',\n",
       "  'discard',\n",
       "  'less',\n",
       "  'import',\n",
       "  'one',\n",
       "  '(',\n",
       "  'new',\n",
       "  'featur',\n",
       "  'creat',\n",
       "  ')',\n",
       "  '?',\n",
       "  '!',\n",
       "  '.'],\n",
       " ['if',\n",
       "  'interest',\n",
       "  'find',\n",
       "  'featur',\n",
       "  'select',\n",
       "  ',',\n",
       "  'find',\n",
       "  'inform',\n",
       "  'previou',\n",
       "  'articl',\n",
       "  '.'],\n",
       " ['in',\n",
       "  'articl',\n",
       "  ',',\n",
       "  'i',\n",
       "  'walk',\n",
       "  'appli',\n",
       "  'featur',\n",
       "  'extract',\n",
       "  'techniqu',\n",
       "  'use',\n",
       "  'kaggl',\n",
       "  'mushroom',\n",
       "  'classif',\n",
       "  'dataset',\n",
       "  'exampl',\n",
       "  '?',\n",
       "  '?',\n",
       "  '?'],\n",
       " ['our',\n",
       "  'object',\n",
       "  'tri',\n",
       "  'predict',\n",
       "  'mushroom',\n",
       "  'poison',\n",
       "  'look',\n",
       "  'given',\n",
       "  'featur',\n",
       "  '.'],\n",
       " ['all', 'code', 'use', 'post', '(', '!', ')'],\n",
       " ['avail', 'kaggl', 'github', 'account', '.']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d3c31",
   "metadata": {},
   "source": [
    "### 4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3bdca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52b46499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Noun\n",
    "lemmatized_noun_words = []\n",
    "for s in stop_word_removed:\n",
    "    tempArr = []\n",
    "    for word in s:\n",
    "        tempArr.append(lem.lemmatize(word,pos='n'))\n",
    "    lemmatized_noun_words.append(tempArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b10fc14f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Feature',\n",
       "  'Extraction',\n",
       "  'aim',\n",
       "  'reduce',\n",
       "  'number',\n",
       "  'feature',\n",
       "  'dataset',\n",
       "  'creating',\n",
       "  'new',\n",
       "  'feature',\n",
       "  'existing',\n",
       "  'one',\n",
       "  '(',\n",
       "  'discarding',\n",
       "  'original',\n",
       "  'feature',\n",
       "  ')',\n",
       "  '.'],\n",
       " ['These',\n",
       "  'new',\n",
       "  'reduced',\n",
       "  'set',\n",
       "  'feature',\n",
       "  'able',\n",
       "  'summarize',\n",
       "  'information',\n",
       "  'contained',\n",
       "  'original',\n",
       "  'set',\n",
       "  'feature',\n",
       "  '!',\n",
       "  '!',\n",
       "  '!',\n",
       "  '.'],\n",
       " ['In',\n",
       "  'way',\n",
       "  ',',\n",
       "  'summarised',\n",
       "  'version',\n",
       "  'original',\n",
       "  'feature',\n",
       "  'created',\n",
       "  'combination',\n",
       "  'original',\n",
       "  'set',\n",
       "  '!',\n",
       "  '!',\n",
       "  '!'],\n",
       " ['Another',\n",
       "  'commonly',\n",
       "  'used',\n",
       "  'technique',\n",
       "  'reduce',\n",
       "  'number',\n",
       "  'feature',\n",
       "  'dataset',\n",
       "  'Feature',\n",
       "  'Selection',\n",
       "  '!'],\n",
       " ['The',\n",
       "  'difference',\n",
       "  'Feature',\n",
       "  'Selection',\n",
       "  'and/or',\n",
       "  'Feature',\n",
       "  'Extraction',\n",
       "  'feature',\n",
       "  'selection',\n",
       "  'aim',\n",
       "  'instead',\n",
       "  '$',\n",
       "  'rank',\n",
       "  'importance',\n",
       "  'existing',\n",
       "  'feature',\n",
       "  'dataset',\n",
       "  'discard',\n",
       "  'le',\n",
       "  'important',\n",
       "  'one',\n",
       "  '(',\n",
       "  'new',\n",
       "  'feature',\n",
       "  'created',\n",
       "  ')',\n",
       "  '?',\n",
       "  '!',\n",
       "  '.'],\n",
       " ['If',\n",
       "  'interested',\n",
       "  'finding',\n",
       "  'Feature',\n",
       "  'Selection',\n",
       "  ',',\n",
       "  'find',\n",
       "  'information',\n",
       "  'previous',\n",
       "  'article',\n",
       "  '.'],\n",
       " ['In',\n",
       "  'article',\n",
       "  ',',\n",
       "  'I',\n",
       "  'walk',\n",
       "  'apply',\n",
       "  'Feature',\n",
       "  'Extraction',\n",
       "  'technique',\n",
       "  'using',\n",
       "  'Kaggle',\n",
       "  'Mushroom',\n",
       "  'Classification',\n",
       "  'Dataset',\n",
       "  'example',\n",
       "  '?',\n",
       "  '?',\n",
       "  '?'],\n",
       " ['Our',\n",
       "  'objective',\n",
       "  'try',\n",
       "  'predict',\n",
       "  'Mushroom',\n",
       "  'poisonous',\n",
       "  'looking',\n",
       "  'given',\n",
       "  'feature',\n",
       "  '.'],\n",
       " ['All', 'code', 'used', 'post', '(', '!', ')'],\n",
       " ['available', 'Kaggle', 'GitHub', 'Account', '.']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_noun_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2751d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Verb\n",
    "lemmatized_verb_words = []\n",
    "for s in stop_word_removed:\n",
    "    tempArr = []\n",
    "    for word in s:\n",
    "        tempArr.append(lem.lemmatize(word,pos='v'))\n",
    "    lemmatized_verb_words.append(tempArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9fca723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Feature',\n",
       "  'Extraction',\n",
       "  'aim',\n",
       "  'reduce',\n",
       "  'number',\n",
       "  'feature',\n",
       "  'dataset',\n",
       "  'create',\n",
       "  'new',\n",
       "  'feature',\n",
       "  'exist',\n",
       "  'ones',\n",
       "  '(',\n",
       "  'discard',\n",
       "  'original',\n",
       "  'feature',\n",
       "  ')',\n",
       "  '.'],\n",
       " ['These',\n",
       "  'new',\n",
       "  'reduce',\n",
       "  'set',\n",
       "  'feature',\n",
       "  'able',\n",
       "  'summarize',\n",
       "  'information',\n",
       "  'contain',\n",
       "  'original',\n",
       "  'set',\n",
       "  'feature',\n",
       "  '!',\n",
       "  '!',\n",
       "  '!',\n",
       "  '.'],\n",
       " ['In',\n",
       "  'way',\n",
       "  ',',\n",
       "  'summarise',\n",
       "  'version',\n",
       "  'original',\n",
       "  'feature',\n",
       "  'create',\n",
       "  'combination',\n",
       "  'original',\n",
       "  'set',\n",
       "  '!',\n",
       "  '!',\n",
       "  '!'],\n",
       " ['Another',\n",
       "  'commonly',\n",
       "  'use',\n",
       "  'technique',\n",
       "  'reduce',\n",
       "  'number',\n",
       "  'feature',\n",
       "  'dataset',\n",
       "  'Feature',\n",
       "  'Selection',\n",
       "  '!'],\n",
       " ['The',\n",
       "  'difference',\n",
       "  'Feature',\n",
       "  'Selection',\n",
       "  'and/or',\n",
       "  'Feature',\n",
       "  'Extraction',\n",
       "  'feature',\n",
       "  'selection',\n",
       "  'aim',\n",
       "  'instead',\n",
       "  '$',\n",
       "  'rank',\n",
       "  'importance',\n",
       "  'exist',\n",
       "  'feature',\n",
       "  'dataset',\n",
       "  'discard',\n",
       "  'less',\n",
       "  'important',\n",
       "  'ones',\n",
       "  '(',\n",
       "  'new',\n",
       "  'feature',\n",
       "  'create',\n",
       "  ')',\n",
       "  '?',\n",
       "  '!',\n",
       "  '.'],\n",
       " ['If',\n",
       "  'interest',\n",
       "  'find',\n",
       "  'Feature',\n",
       "  'Selection',\n",
       "  ',',\n",
       "  'find',\n",
       "  'information',\n",
       "  'previous',\n",
       "  'article',\n",
       "  '.'],\n",
       " ['In',\n",
       "  'article',\n",
       "  ',',\n",
       "  'I',\n",
       "  'walk',\n",
       "  'apply',\n",
       "  'Feature',\n",
       "  'Extraction',\n",
       "  'techniques',\n",
       "  'use',\n",
       "  'Kaggle',\n",
       "  'Mushroom',\n",
       "  'Classification',\n",
       "  'Dataset',\n",
       "  'example',\n",
       "  '?',\n",
       "  '?',\n",
       "  '?'],\n",
       " ['Our',\n",
       "  'objective',\n",
       "  'try',\n",
       "  'predict',\n",
       "  'Mushroom',\n",
       "  'poisonous',\n",
       "  'look',\n",
       "  'give',\n",
       "  'feature',\n",
       "  '.'],\n",
       " ['All', 'code', 'use', 'post', '(', '!', ')'],\n",
       " ['available', 'Kaggle', 'GitHub', 'Account', '.']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_verb_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e614bdf",
   "metadata": {},
   "source": [
    "### 5. POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7482da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "pos_list = []\n",
    "for sent in lemmatized_verb_words:\n",
    "    pos_list.append(nltk.pos_tag(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28a3e87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Feature', 'NN'),\n",
       "  ('Extraction', 'NNP'),\n",
       "  ('aim', 'NN'),\n",
       "  ('reduce', 'VB'),\n",
       "  ('number', 'NN'),\n",
       "  ('feature', 'NN'),\n",
       "  ('dataset', 'VB'),\n",
       "  ('create', 'JJ'),\n",
       "  ('new', 'JJ'),\n",
       "  ('feature', 'NN'),\n",
       "  ('exist', 'VBP'),\n",
       "  ('ones', 'NNS'),\n",
       "  ('(', '('),\n",
       "  ('discard', 'JJ'),\n",
       "  ('original', 'JJ'),\n",
       "  ('feature', 'NN'),\n",
       "  (')', ')'),\n",
       "  ('.', '.')],\n",
       " [('These', 'DT'),\n",
       "  ('new', 'JJ'),\n",
       "  ('reduce', 'VB'),\n",
       "  ('set', 'VBN'),\n",
       "  ('feature', 'NN'),\n",
       "  ('able', 'JJ'),\n",
       "  ('summarize', 'JJ'),\n",
       "  ('information', 'NN'),\n",
       "  ('contain', 'NN'),\n",
       "  ('original', 'JJ'),\n",
       "  ('set', 'NN'),\n",
       "  ('feature', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('!', '.'),\n",
       "  ('!', '.'),\n",
       "  ('.', '.')],\n",
       " [('In', 'IN'),\n",
       "  ('way', 'NN'),\n",
       "  (',', ','),\n",
       "  ('summarise', 'VB'),\n",
       "  ('version', 'NN'),\n",
       "  ('original', 'JJ'),\n",
       "  ('feature', 'NN'),\n",
       "  ('create', 'NN'),\n",
       "  ('combination', 'NN'),\n",
       "  ('original', 'JJ'),\n",
       "  ('set', 'NN'),\n",
       "  ('!', '.'),\n",
       "  ('!', '.'),\n",
       "  ('!', '.')],\n",
       " [('Another', 'DT'),\n",
       "  ('commonly', 'RB'),\n",
       "  ('use', 'NN'),\n",
       "  ('technique', 'NN'),\n",
       "  ('reduce', 'VB'),\n",
       "  ('number', 'NN'),\n",
       "  ('feature', 'NN'),\n",
       "  ('dataset', 'VBN'),\n",
       "  ('Feature', 'NNP'),\n",
       "  ('Selection', 'NNP'),\n",
       "  ('!', '.')],\n",
       " [('The', 'DT'),\n",
       "  ('difference', 'NN'),\n",
       "  ('Feature', 'NNP'),\n",
       "  ('Selection', 'NNP'),\n",
       "  ('and/or', 'VBZ'),\n",
       "  ('Feature', 'NNP'),\n",
       "  ('Extraction', 'NNP'),\n",
       "  ('feature', 'NN'),\n",
       "  ('selection', 'NN'),\n",
       "  ('aim', 'NN'),\n",
       "  ('instead', 'RB'),\n",
       "  ('$', '$'),\n",
       "  ('rank', 'JJ'),\n",
       "  ('importance', 'NN'),\n",
       "  ('exist', 'NN'),\n",
       "  ('feature', 'NN'),\n",
       "  ('dataset', 'VBN'),\n",
       "  ('discard', 'RB'),\n",
       "  ('less', 'RBR'),\n",
       "  ('important', 'JJ'),\n",
       "  ('ones', 'NNS'),\n",
       "  ('(', '('),\n",
       "  ('new', 'JJ'),\n",
       "  ('feature', 'NN'),\n",
       "  ('create', 'NN'),\n",
       "  (')', ')'),\n",
       "  ('?', '.'),\n",
       "  ('!', '.'),\n",
       "  ('.', '.')],\n",
       " [('If', 'IN'),\n",
       "  ('interest', 'NN'),\n",
       "  ('find', 'VBP'),\n",
       "  ('Feature', 'NNP'),\n",
       "  ('Selection', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('find', 'VB'),\n",
       "  ('information', 'NN'),\n",
       "  ('previous', 'JJ'),\n",
       "  ('article', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('In', 'IN'),\n",
       "  ('article', 'NN'),\n",
       "  (',', ','),\n",
       "  ('I', 'PRP'),\n",
       "  ('walk', 'VBP'),\n",
       "  ('apply', 'JJ'),\n",
       "  ('Feature', 'NNP'),\n",
       "  ('Extraction', 'NNP'),\n",
       "  ('techniques', 'NNS'),\n",
       "  ('use', 'VBP'),\n",
       "  ('Kaggle', 'NNP'),\n",
       "  ('Mushroom', 'NNP'),\n",
       "  ('Classification', 'NNP'),\n",
       "  ('Dataset', 'NNP'),\n",
       "  ('example', 'NN'),\n",
       "  ('?', '.'),\n",
       "  ('?', '.'),\n",
       "  ('?', '.')],\n",
       " [('Our', 'PRP$'),\n",
       "  ('objective', 'JJ'),\n",
       "  ('try', 'NN'),\n",
       "  ('predict', 'NN'),\n",
       "  ('Mushroom', 'NNP'),\n",
       "  ('poisonous', 'JJ'),\n",
       "  ('look', 'NN'),\n",
       "  ('give', 'JJ'),\n",
       "  ('feature', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('All', 'DT'),\n",
       "  ('code', 'NN'),\n",
       "  ('use', 'NN'),\n",
       "  ('post', 'NN'),\n",
       "  ('(', '('),\n",
       "  ('!', '.'),\n",
       "  (')', ')')],\n",
       " [('available', 'JJ'),\n",
       "  ('Kaggle', 'NNP'),\n",
       "  ('GitHub', 'NNP'),\n",
       "  ('Account', 'NNP'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a36429",
   "metadata": {},
   "source": [
    "### 6. Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe52fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining \n",
    "sentenced = []\n",
    "\n",
    "for sent in lemmatized_noun_words:\n",
    "    sentenced.append(' '.join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e782086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Feature Extraction aim reduce number feature dataset creating new feature existing one ( discarding original feature ) .',\n",
       " 'These new reduced set feature able summarize information contained original set feature ! ! ! .',\n",
       " 'In way , summarised version original feature created combination original set ! ! !',\n",
       " 'Another commonly used technique reduce number feature dataset Feature Selection !',\n",
       " 'The difference Feature Selection and/or Feature Extraction feature selection aim instead $ rank importance existing feature dataset discard le important one ( new feature created ) ? ! .',\n",
       " 'If interested finding Feature Selection , find information previous article .',\n",
       " 'In article , I walk apply Feature Extraction technique using Kaggle Mushroom Classification Dataset example ? ? ?',\n",
       " 'Our objective try predict Mushroom poisonous looking given feature .',\n",
       " 'All code used post ( ! )',\n",
       " 'available Kaggle GitHub Account .']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41e4a789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98a6e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = count.fit_transform(sentenced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b65dace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "        1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "        1, 5, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260aa306",
   "metadata": {},
   "source": [
    "### 7.Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fab1fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e043e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = tfid.fit_transform(sentenced).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3b0e93d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.2579254 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.30340904, 0.2006228 , 0.        , 0.        , 0.30340904,\n",
       "        0.        , 0.2579254 , 0.22565425, 0.53874791, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.22565425,\n",
       "        0.2579254 , 0.        , 0.2579254 , 0.        , 0.22565425,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.2579254 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.30849249, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.30849249, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.27388717, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.2622468 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.22943496,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.22943496,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.30849249, 0.        , 0.5244936 ,\n",
       "        0.        , 0.30849249, 0.        , 0.        , 0.30849249,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.34144296, 0.        , 0.        , 0.29025771,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.1515707 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.29025771, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.5078824 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.29025771,\n",
       "        0.34144296, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.34144296, 0.        ,\n",
       "        0.34144296],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.38722446, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.38722446, 0.        , 0.        ,\n",
       "        0.        , 0.25604397, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.34378733, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.32917617, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.32917617, 0.        , 0.28799025, 0.        ,\n",
       "        0.        , 0.        , 0.32917617, 0.        , 0.        ,\n",
       "        0.        , 0.32917617, 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.18302455, 0.        , 0.21529986,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18302455,\n",
       "        0.        , 0.14236247, 0.21529986, 0.21529986, 0.        ,\n",
       "        0.        , 0.18302455, 0.16012485, 0.47787118, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.21529986,\n",
       "        0.21529986, 0.        , 0.        , 0.21529986, 0.        ,\n",
       "        0.        , 0.21529986, 0.        , 0.        , 0.16012485,\n",
       "        0.        , 0.        , 0.18302455, 0.21529986, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.21529986, 0.        , 0.        , 0.32024971, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.21529986, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.31690935, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.16548801, 0.37279445,\n",
       "        0.37279445, 0.        , 0.        , 0.37279445, 0.        ,\n",
       "        0.        , 0.        , 0.31690935, 0.        , 0.37279445,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.37279445,\n",
       "        0.        , 0.        , 0.        , 0.27725823, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.31942701, 0.27154214, 0.        , 0.31942701,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.21121434, 0.        , 0.        , 0.        ,\n",
       "        0.31942701, 0.        , 0.23756729, 0.14179755, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.27154214, 0.        , 0.        , 0.        ,\n",
       "        0.27154214, 0.        , 0.        , 0.27154214, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.27154214, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.31942701, 0.        , 0.31942701,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.15774019, 0.        ,\n",
       "        0.        , 0.        , 0.35534096, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.35534096, 0.30207228, 0.        ,\n",
       "        0.        , 0.35534096, 0.        , 0.        , 0.        ,\n",
       "        0.35534096, 0.35534096, 0.        , 0.35534096, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.35534096, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.5182909 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5182909 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.5182909 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.44059462, 0.        , 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.5182909 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.5182909 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.5182909 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.44059462, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2d1092a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 66)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20281ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
